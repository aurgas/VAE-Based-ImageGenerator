{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0597d6d5-91f2-4bb8-9745-df845b480f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from typing import Tuple\n",
    "\n",
    "import importlib\n",
    "import resnet_vae\n",
    "importlib.reload(resnet_vae)\n",
    "\n",
    "from resnet_vae import ResNetVAE, vae_loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b4da270-66b6-4223-bc3a-71585193b0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Uses torchvision ResNet as encoder and returns a spatial feature map.\n",
    "    If pretrained=True, downloads imagenet weights via torchvision.\n",
    "    output shape (B, C_enc, H/32, W/32) for typical ResNets (resnet18/34/50).\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone: str = \"resnet50\", pretrained: bool = True, replace_stride_with_dilation=None):\n",
    "        super().__init__()\n",
    "        assert backbone in (\"resnet18\", \"resnet34\", \"resnet50\"), \"supported: resnet18, resnet34, resnet50\"\n",
    "        if backbone == \"resnet18\":\n",
    "            base = models.resnet18(pretrained=pretrained, replace_stride_with_dilation=replace_stride_with_dilation)\n",
    "            out_channels = 512\n",
    "        elif backbone == \"resnet34\":\n",
    "            base = models.resnet34(pretrained=pretrained, replace_stride_with_dilation=replace_stride_with_dilation)\n",
    "            out_channels = 512\n",
    "        else:\n",
    "            base = models.resnet50(pretrained=pretrained, replace_stride_with_dilation=replace_stride_with_dilation)\n",
    "            out_channels = 2048\n",
    "\n",
    "        # Keep the layers up to layer4 (exclude avgpool & fc)\n",
    "        # We'll reuse conv1, bn1, relu, maxpool, layer1..layer4\n",
    "        self.stem = nn.Sequential(\n",
    "            base.conv1,\n",
    "            base.bn1,\n",
    "            base.relu,\n",
    "            base.maxpool\n",
    "        )\n",
    "        self.layer1 = base.layer1\n",
    "        self.layer2 = base.layer2\n",
    "        self.layer3 = base.layer3\n",
    "        self.layer4 = base.layer4\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (B, 3, H, W)\n",
    "        returns: (B, out_channels, H/32, W/32) for default ResNet strides\n",
    "        \"\"\"\n",
    "        x = self.stem(x)      # /4 then maxpool => /4\n",
    "        x = self.layer1(x)    # /4\n",
    "        x = self.layer2(x)    # /8\n",
    "        x = self.layer3(x)    # /16\n",
    "        x = self.layer4(x)    # /32\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d949fe8-a8a6-489e-9be4-bb4d939ff756",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvDecoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels=3):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        ch = in_channels\n",
    "        for _ in range(5):  # upsample ×2 five times (32× → 1×)\n",
    "            layers += [\n",
    "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "                nn.Conv2d(ch, max(ch // 2, 64), 3, padding=1),\n",
    "                nn.BatchNorm2d(max(ch // 2, 64)),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            ch = max(ch // 2, 64)\n",
    "        layers += [nn.Conv2d(ch, out_channels, 3, padding=1), nn.Sigmoid()]\n",
    "        self.decoder = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.decoder(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bf7a4e5-0c87-4f62-b485-d9b253f15e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetVAE(nn.Module):\n",
    "    def __init__(self, backbone=\"resnet18\", pretrained=True, latent_dim=256):\n",
    "        super().__init__()\n",
    "        self.encoder = ResNetEncoder(backbone=backbone, pretrained=pretrained)\n",
    "        self.enc_out_ch = self.encoder.out_channels\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Project encoder output to latent mean & logvar\n",
    "        self.fc_mu = nn.Linear(self.enc_out_ch * 8 * 8, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(self.enc_out_ch * 8 * 8, latent_dim)\n",
    "\n",
    "        # Project latent back to feature map for decoder\n",
    "        self.fc_decode = nn.Linear(latent_dim, self.enc_out_ch * 8 * 8)\n",
    "\n",
    "        self.decoder = ConvDecoder(self.enc_out_ch, out_channels=3)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        b = x.size(0)\n",
    "        h = self.encoder(x)  # (B, C, 8, 8) for 256×256 inputs\n",
    "        h_flat = h.view(b, -1)\n",
    "\n",
    "        mu = self.fc_mu(h_flat)\n",
    "        logvar = self.fc_logvar(h_flat)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "\n",
    "        # decode\n",
    "        h_dec = self.fc_decode(z).view(b, self.enc_out_ch, 8, 8)\n",
    "        recon = self.decoder(h_dec)\n",
    "        return recon, mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df2fa44e-bcd9-4126-9594-83767cd6c95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss_function(recon_x, x, mu, logvar):\n",
    "    recon_loss = F.mse_loss(recon_x, x, reduction='mean')\n",
    "    # KL divergence: 0.5 * sum(μ² + σ² - logσ² - 1)\n",
    "    kld = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + 0.001 * kld, recon_loss, kld\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cda4a8a1-3379-47bc-8eab-12425e65f2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training step: total=0.0926, rec=0.0920, kl=0.6329\n",
      "Generated images: torch.Size([8, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    vae = ResNetVAE(backbone=\"resnet18\", pretrained=True, latent_dim=256).to(device)\n",
    "    opt = torch.optim.Adam(vae.parameters(), lr=1e-4)\n",
    "\n",
    "    dummy = torch.rand(4, 3, 256, 256).to(device)\n",
    "    recon, mu, logvar = vae(dummy)\n",
    "    loss, rec, kld = vae_loss_function(recon, dummy, mu, logvar)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    print(f\"Training step: total={loss.item():.4f}, rec={rec.item():.4f}, kl={kld.item():.4f}\")\n",
    "\n",
    "    # Generate new images\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(8, vae.latent_dim).to(device)\n",
    "        dec = vae.fc_decode(z).view(-1, vae.enc_out_ch, 8, 8)\n",
    "        samples = vae.decoder(dec)\n",
    "    print(\"Generated images:\", samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c6c1ce5-14f0-4159-8717-ee01bfd02ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "dataset = datasets.ImageFolder(root=\"/Users/poulam/Desktop/Generative Bullshit/archive2/Hands\", transform=transform)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "100bea01-931c-4869-a1a7-5dfbf32d9a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from resnet_vae import ResNetVAE\n",
    "\n",
    "model = ResNetVAE(latent_dim=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01fbab29-ad2a-40c4-ab45-97b9db24e517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import importlib\n",
    "sys.path.append(\"/Users/poulam/Desktop/Generative Bullshit\")\n",
    "import resnet_vae\n",
    "importlib.reload(resnet_vae)\n",
    "from resnet_vae import ResNetVAE, vae_loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0a52ec7-10bc-42ff-9199-8566ff000559",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from resnet_vae import ResNetVAE, vae_loss_function\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "vae = ResNetVAE(backbone=\"resnet18\", pretrained=True, latent_dim=256, latent_spatial=8).to(device)\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ffce5cb-8869-4c79-9323-c43be88e31df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|█| 1385/1385 [22:50<00:00,  1.01it/s, Total=8753.78, Recon=7354\n",
      "Epoch 2/50: 100%|█| 1385/1385 [22:46<00:00,  1.01it/s, Total=6532.63, Recon=5288\n",
      "Epoch 3/50: 100%|█| 1385/1385 [24:19<00:00,  1.05s/it, Total=5944.91, Recon=4863\n",
      "Epoch 4/50: 100%|█| 1385/1385 [25:41<00:00,  1.11s/it, Total=3593.92, Recon=2800\n",
      "Epoch 5/50: 100%|█| 1385/1385 [3:03:14<00:00,  7.94s/it, Total=3478.49, Recon=24\n",
      "Epoch 6/50: 100%|█| 1385/1385 [20:54<00:00,  1.10it/s, Total=3778.11, Recon=2970\n",
      "Epoch 7/50: 100%|█| 1385/1385 [20:43<00:00,  1.11it/s, Total=3396.31, Recon=2742\n",
      "Epoch 8/50: 100%|█| 1385/1385 [20:24<00:00,  1.13it/s, Total=2558.42, Recon=1957\n",
      "Epoch 9/50: 100%|█| 1385/1385 [38:33<00:00,  1.67s/it, Total=2370.21, Recon=1741\n",
      "Epoch 10/50: 100%|█| 1385/1385 [22:13<00:00,  1.04it/s, Total=2831.01, Recon=214\n",
      "Epoch 11/50: 100%|█| 1385/1385 [23:46<00:00,  1.03s/it, Total=2294.37, Recon=159\n",
      "Epoch 12/50: 100%|█| 1385/1385 [22:39<00:00,  1.02it/s, Total=2399.12, Recon=170\n",
      "Epoch 13/50: 100%|█| 1385/1385 [22:45<00:00,  1.01it/s, Total=2303.90, Recon=168\n",
      "Epoch 14/50: 100%|█| 1385/1385 [22:59<00:00,  1.00it/s, Total=2787.96, Recon=206\n",
      "Epoch 15/50: 100%|█| 1385/1385 [22:58<00:00,  1.00it/s, Total=2270.13, Recon=158\n",
      "Epoch 16/50: 100%|█| 1385/1385 [22:57<00:00,  1.01it/s, Total=2165.34, Recon=152\n",
      "Epoch 17/50: 100%|█| 1385/1385 [22:16<00:00,  1.04it/s, Total=2040.72, Recon=146\n",
      "Epoch 18/50: 100%|█| 1385/1385 [23:15<00:00,  1.01s/it, Total=2408.54, Recon=156\n",
      "Epoch 19/50: 100%|█| 1385/1385 [22:39<00:00,  1.02it/s, Total=1628.51, Recon=106\n",
      "Epoch 20/50: 100%|█| 1385/1385 [21:37<00:00,  1.07it/s, Total=1713.96, Recon=113\n",
      "Epoch 21/50: 100%|█| 1385/1385 [21:34<00:00,  1.07it/s, Total=2040.71, Recon=134\n",
      "Epoch 22/50: 100%|█| 1385/1385 [21:56<00:00,  1.05it/s, Total=1955.74, Recon=135\n",
      "Epoch 23/50: 100%|█| 1385/1385 [22:09<00:00,  1.04it/s, Total=1572.84, Recon=105\n",
      "Epoch 24/50: 100%|█| 1385/1385 [23:01<00:00,  1.00it/s, Total=1746.10, Recon=113\n",
      "Epoch 25/50: 100%|█| 1385/1385 [21:57<00:00,  1.05it/s, Total=1565.07, Recon=105\n",
      "Epoch 26/50: 100%|█| 1385/1385 [21:34<00:00,  1.07it/s, Total=1917.94, Recon=133\n",
      "Epoch 27/50: 100%|█| 1385/1385 [21:31<00:00,  1.07it/s, Total=1369.22, Recon=891\n",
      "Epoch 28/50: 100%|█| 1385/1385 [21:07<00:00,  1.09it/s, Total=1806.04, Recon=119\n",
      "Epoch 29/50: 100%|█| 1385/1385 [21:26<00:00,  1.08it/s, Total=1739.11, Recon=116\n",
      "Epoch 30/50: 100%|█| 1385/1385 [22:37<00:00,  1.02it/s, Total=1672.59, Recon=106\n",
      "Epoch 31/50: 100%|█| 1385/1385 [22:27<00:00,  1.03it/s, Total=1615.10, Recon=105\n",
      "Epoch 32/50: 100%|█| 1385/1385 [22:08<00:00,  1.04it/s, Total=1543.60, Recon=104\n",
      "Epoch 33/50: 100%|█| 1385/1385 [22:03<00:00,  1.05it/s, Total=1676.85, Recon=101\n",
      "Epoch 34/50: 100%|█| 1385/1385 [22:08<00:00,  1.04it/s, Total=1745.82, Recon=114\n",
      "Epoch 35/50: 100%|█| 1385/1385 [22:09<00:00,  1.04it/s, Total=1375.13, Recon=848\n",
      "Epoch 36/50: 100%|█| 1385/1385 [22:17<00:00,  1.04it/s, Total=1932.75, Recon=142\n",
      "Epoch 37/50: 100%|█| 1385/1385 [22:24<00:00,  1.03it/s, Total=1453.08, Recon=893\n",
      "Epoch 38/50: 100%|█| 1385/1385 [22:24<00:00,  1.03it/s, Total=1320.41, Recon=809\n",
      "Epoch 39/50: 100%|█| 1385/1385 [22:31<00:00,  1.02it/s, Total=1460.50, Recon=942\n",
      "Epoch 40/50: 100%|█| 1385/1385 [22:03<00:00,  1.05it/s, Total=1324.10, Recon=826\n",
      "Epoch 41/50: 100%|█| 1385/1385 [22:17<00:00,  1.04it/s, Total=1269.23, Recon=741\n",
      "Epoch 42/50: 100%|█| 1385/1385 [22:21<00:00,  1.03it/s, Total=1798.04, Recon=122\n",
      "Epoch 43/50: 100%|█| 1385/1385 [22:23<00:00,  1.03it/s, Total=1452.30, Recon=924\n",
      "Epoch 44/50: 100%|█| 1385/1385 [22:22<00:00,  1.03it/s, Total=1670.16, Recon=113\n",
      "Epoch 45/50: 100%|█| 1385/1385 [22:25<00:00,  1.03it/s, Total=1331.99, Recon=812\n",
      "Epoch 46/50: 100%|█| 1385/1385 [22:34<00:00,  1.02it/s, Total=1803.37, Recon=130\n",
      "Epoch 47/50: 100%|█| 1385/1385 [22:30<00:00,  1.03it/s, Total=1389.99, Recon=820\n",
      "Epoch 48/50: 100%|█| 1385/1385 [22:36<00:00,  1.02it/s, Total=1636.58, Recon=107\n",
      "Epoch 49/50: 100%|█| 1385/1385 [21:55<00:00,  1.05it/s, Total=1359.30, Recon=865\n",
      "Epoch 50/50: 100%|█| 1385/1385 [22:31<00:00,  1.03it/s, Total=1772.92, Recon=121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from resnet_vae import ResNetVAE, vae_loss_function\n",
    "\n",
    "# assume you already have dataset and dataloader\n",
    "for epoch in range(50):\n",
    "    vae.train()\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/50\")\n",
    "    for imgs, _ in pbar:\n",
    "        imgs = imgs.to(device)\n",
    "        recon, mu, logvar = vae(imgs)\n",
    "        loss, rec_loss, kl_loss = vae_loss_function(recon, imgs, mu, logvar)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            \"Total\": f\"{loss.item():.2f}\",\n",
    "            \"Recon\": f\"{rec_loss.item():.2f}\",\n",
    "            \"KL\": f\"{kl_loss.item():.2f}\"\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e99ba10-aa68-444d-97f6-7bdbb112cf50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 3, 256, 256)\n",
    "out = vae.encoder(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c68b532e-0b15-498e-ab27-d80cfa06ef95",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vae.state_dict(), \"vae_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "14d6c8e6-add9-4f1f-a900-886adecaf613",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ResNetVAE' object has no attribute 'latent_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m vae\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Generate a random latent vector\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatent_dim\u001b[49m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Decode it to an image\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1928\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1926\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1927\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1928\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1930\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ResNetVAE' object has no attribute 'latent_dim'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.utils import save_image\n",
    "from resnet_vae import ResNetVAE\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load your trained VAE model\n",
    "vae = ResNetVAE(latent_dim=256).to(device)\n",
    "vae.load_state_dict(torch.load(\"/Users/poulam/Desktop/Generative Bullshit/vae_weights.pth\", map_location=device))\n",
    "vae.eval()\n",
    "\n",
    "# Generate a random latent vector\n",
    "z = torch.randn(1, vae.latent_dim).to(device)\n",
    "\n",
    "# Decode it to an image\n",
    "with torch.no_grad():\n",
    "    generated = vae.decode(z)\n",
    "\n",
    "# Save the generated image to your project directory\n",
    "save_path = \"/Users/poulam/Desktop/Generative Bullshit/generated_sample.png\"\n",
    "save_image(generated, save_path)\n",
    "\n",
    "print(f\"✅ Image successfully saved at: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beeef6e-3018-44fe-b8d7-decc60f9cf3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
