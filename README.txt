# ğŸ§  ResNet-Diffusion VAE  
### Hybrid Latent Image Generator using ResNet Encoder + Diffusion Decoder

This project implements a **hybrid generative model** that combines a **pretrained ResNet encoder** with a **diffusion-based decoder** (UNet architecture) for image generation.  
Inspired by *Stable Diffusion*, this design replaces the traditional VAE decoder with a pretrained **denoising diffusion model**, allowing for sharper and more diverse image synthesis.

---

## ğŸš€ Features

- ğŸ§© **ResNet Encoder** â€“ Pretrained ResNet18/34/50 backbone for feature extraction.  
- ğŸŒ«ï¸ **Diffusion Decoder** â€“ Pretrained UNet (from ğŸ¤— Hugging Face Diffusers) as the denoising decoder.  
- ğŸ§  **Variational Latent Space** â€“ VAE-style latent sampling using mean & variance projections.  
- ğŸ”„ **Conditional Generation** â€“ Image generation conditioned on learned latent embeddings.  
- âš¡ **Supports both pixel-space and latent-space diffusion**  
- ğŸ’¾ **Modular Design** â€“ Easy to swap in different backbones or diffusion models.

---

## ğŸ§± Project Structure

ResNet-Diffusion-VAE/
â”‚
â”œâ”€â”€ resnet_vae.py               # ResNet-based VAE model (ResNet encoder + transpose decoder)
â”œâ”€â”€ resnet_diffusion_vae.py     # Hybrid model using ResNet encoder + diffusion decoder
â”œâ”€â”€ README.md                   # Project documentation (this file)
â”œâ”€â”€ requirements.txt            # Required Python libraries
â”œâ”€â”€ vae_weights.pth             # (optional) Trained VAE checkpoint
â””â”€â”€ generated_sample.png        # Example generated output

---

## ğŸ§© Model Architecture

### 1. Encoder  
- Uses **ResNet18** pretrained on ImageNet (`torchvision.models.resnet18`)  
- Extracts high-level features â†’ outputs mean (`Î¼`) and log variance (`logÏƒÂ²`) vectors  
- Produces a latent embedding `z = Î¼ + Ïƒ âŠ™ Îµ`

### 2. Diffusion Decoder  
- Based on a pretrained **Stable Diffusion UNet** (`UNet2DConditionModel` from `diffusers`)  
- Denoises Gaussian noise into a final image, conditioned on `z`  
- Uses a small adapter MLP to project `z` into the UNetâ€™s **cross-attention space (768-dim)**

---

## ğŸ§  How It Works

1. **Encode an Image**  
   - Input â†’ ResNet Encoder â†’ latent vector `z` (256-dim default)  
2. **Condition the Diffusion Decoder**  
   - Project `z` â†’ 768-dim cross-attention embedding  
3. **Generate Image**  
   - Start from random noise â†’ Diffusion UNet denoises over multiple timesteps  
   - Final output: synthetic image consistent with encoded latent distribution  

---

## ğŸ§ª Example Usage

### Generate a Random Image
```bash
python resnet_diffusion_vae.py
```

### Conditional Generation (from image)
In `resnet_diffusion_vae.py`, uncomment:
```python
generated = generate_from_resnet(image_path="path/to/image.jpg", num_steps=50)
```

This will encode the given image â†’ latent `z` â†’ condition diffusion model â†’ reconstruct a new variant.

---

## âš™ï¸ Installation

```bash
git clone https://github.com/<your-username>/ResNet-Diffusion-VAE.git
cd ResNet-Diffusion-VAE

# Create a virtual environment (recommended)
python -m venv venv
source venv/bin/activate   # on Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

---

## ğŸ§© Requirements

See [`requirements.txt`](./requirements.txt)

Main libraries:
- torch, torchvision
- diffusers
- transformers
- accelerate
- safetensors
- Pillow
- tqdm

---

## ğŸ§° Notes

- The first run will **download pretrained diffusion weights (~3.4 GB)** from Hugging Face.  
- Once downloaded, theyâ€™re cached in `~/.cache/huggingface/`.  
- Works with both CPU and CUDA (GPU recommended).  
- To reduce VRAM usage, enable half-precision (`torch.float16`) inference.

---

## ğŸ“ˆ Future Improvements

- [ ] Add VAE fine-tuning with diffusion guidance  
- [ ] Integrate Stable Diffusion VAE latent-space decoder  
- [ ] Add ControlNet or LoRA conditioning adapters  
- [ ] Support for text + image joint conditioning  

---

## ğŸ§‘â€ğŸ’» Author

**Poulam Saha**  
Generative AI & Deep Learning Enthusiast  
ğŸ“ India  
ğŸŒ *GitHub*: [github.com/<your-username>](https://github.com/<your-username>)  

---

## ğŸªª License

MIT License Â© 2025 Poulam Saha
